"""
Evaluate generated code using ParEval benchmark framework.

This script wraps ParEval's evaluation logic to compile, run, and compute metrics
for code generated by MoT. It imports ParEval modules directly to reuse the
existing test infrastructure.

Usage:
    cd ~/mot/pareval
    python evaluate_pareval.py --input ./outputs/mot_generation.json

    # With custom output path
    python evaluate_pareval.py --input ./outputs/mot_generation.json \
        --output ./outputs/mot_generation_results.json

    # Skip compilation/running, only compute metrics from existing results
    python evaluate_pareval.py --input ./outputs/mot_generation_results.json \
        --metrics_only
"""

import os
import sys
import json
import argparse
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime

# =============================================================================
# Configuration
# =============================================================================
# Get the directory where this script is located
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# Drivers directory is now local to this script (mot/pareval/drivers/)
DRIVERS_DIR = os.path.join(SCRIPT_DIR, "drivers")

# Default paths relative to local drivers directory
DEFAULT_LAUNCH_CONFIGS = os.path.join(DRIVERS_DIR, "launch-configs.json")
DEFAULT_BUILD_CONFIGS = os.path.join(DRIVERS_DIR, "build-configs.json")
DEFAULT_PROBLEM_SIZES = os.path.join(DRIVERS_DIR, "problem-sizes.json")

# Local launch configuration (without SLURM/srun)
# This is used when --local flag is set or srun is not available
LOCAL_LAUNCH_CONFIGS = {
    "cuda": {
        "format": "{exec_path} {args}",
        "params": [{}]
    },
    "hip": {
        "format": "{exec_path} {args}",
        "params": [{}]
    },
    "serial": {
        "format": "{exec_path} {args}",
        "params": [{}]
    },
    "omp": {
        "format": "OMP_NUM_THREADS={num_threads} {exec_path} {args}",
        "params": [
            {"num_threads": 1},
            {"num_threads": 2},
            {"num_threads": 4},
            {"num_threads": 8}
        ]
    },
    "mpi": {
        "format": "mpirun -np {num_procs} {exec_path} {args}",
        "params": [
            {"num_procs": 1},
            {"num_procs": 2},
            {"num_procs": 4}
        ]
    },
    "mpi+omp": {
        "format": "OMP_NUM_THREADS={num_threads} mpirun -np {num_procs} {exec_path} {args}",
        "params": [
            {"num_procs": 2, "num_threads": 2},
            {"num_procs": 2, "num_threads": 4}
        ]
    },
    "kokkos": {
        "format": "{exec_path} {args}",
        "params": [
            {"num_threads": 1},
            {"num_threads": 4},
            {"num_threads": 8}
        ]
    }
}


def setup_local_imports():
    """
    Setup sys.path to import local ParEval driver modules.
    The drivers directory should be at mot/pareval/drivers/
    """
    # Add drivers path to sys.path for imports like "from cpp.xxx import yyy"
    if DRIVERS_DIR not in sys.path:
        sys.path.insert(0, DRIVERS_DIR)
    
    # Also add SCRIPT_DIR in case there are imports relative to that
    if SCRIPT_DIR not in sys.path:
        sys.path.insert(0, SCRIPT_DIR)
    
    # Verify drivers directory exists
    if not os.path.exists(DRIVERS_DIR):
        raise FileNotFoundError(
            f"Drivers directory not found at {DRIVERS_DIR}\n"
            f"Please ensure the ParEval drivers folder has been copied to mot/pareval/drivers/"
        )
    
    # Verify key files exist
    required_files = [
        os.path.join(DRIVERS_DIR, "cpp", "cpp_driver_wrapper.py"),
        os.path.join(DRIVERS_DIR, "util.py"),
    ]
    
    for filepath in required_files:
        if not os.path.exists(filepath):
            raise FileNotFoundError(
                f"Required file not found: {filepath}\n"
                f"Please ensure all ParEval driver files have been copied correctly."
            )


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description='Evaluate generated code using ParEval benchmark',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Input/Output arguments
    parser.add_argument('--input', type=str, required=True,
                        help='Path to generated code JSON file')
    parser.add_argument('--output', type=str, default=None,
                        help='Path to output results JSON file (default: input_results.json)')
    parser.add_argument('--metrics_output', type=str, default=None,
                        help='Path to output metrics JSON file (default: input_metrics.json)')
    
    # ParEval configuration (now using local paths by default)
    parser.add_argument('--drivers_dir', type=str, default=DRIVERS_DIR,
                        help='Path to local drivers directory')
    parser.add_argument('--launch_configs', type=str, default=None,
                        help='Path to launch configs JSON')
    parser.add_argument('--build_configs', type=str, default=None,
                        help='Path to build configs JSON')
    parser.add_argument('--problem_sizes', type=str, default=None,
                        help='Path to problem sizes JSON')
    
    # Evaluation options
    parser.add_argument('--metrics_only', action='store_true',
                        help='Skip compilation/running, only compute metrics from results')
    parser.add_argument('--skip_metrics', action='store_true',
                        help='Skip metrics computation after evaluation')
    parser.add_argument('--local', action='store_true', default=True,
                        help='Use local execution (no SLURM/srun). Default: True')
    parser.add_argument('--use_slurm', action='store_true',
                        help='Use SLURM launch configs from ParEval (requires srun)')
    parser.add_argument('--build_timeout', type=int, default=60,
                        help='Timeout in seconds for building a program')
    parser.add_argument('--run_timeout', type=int, default=120,
                        help='Timeout in seconds for running a program')
    parser.add_argument('--scratch_dir', type=str, default=None,
                        help='Directory for temporary build files')
    
    # Logging options
    parser.add_argument('--log_level', type=str, default='INFO',
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                        help='Logging level')
    parser.add_argument('--log_build_errors', action='store_true',
                        help='Display build errors in detail')
    
    # Metric computation options
    parser.add_argument('--k_values', type=int, nargs='+', default=[1, 5, 10],
                        help='K values for pass@k computation')
    parser.add_argument('--model_name', type=str, default='MoT-CUDA',
                        help='Model name for metrics output')
    
    args = parser.parse_args()
    
    # Set default output paths
    if args.output is None:
        input_base = os.path.splitext(args.input)[0]
        args.output = f"{input_base}_results.json"
    
    if args.metrics_output is None:
        input_base = os.path.splitext(args.input)[0]
        args.metrics_output = f"{input_base}_metrics.json"
    
    # Set default config paths using local drivers directory
    drivers_dir = args.drivers_dir
    if args.launch_configs is None:
        args.launch_configs = os.path.join(drivers_dir, "launch-configs.json")
    if args.build_configs is None:
        args.build_configs = os.path.join(drivers_dir, "build-configs.json")
    if args.problem_sizes is None:
        args.problem_sizes = os.path.join(drivers_dir, "problem-sizes.json")
    
    return args


def load_json(filepath: str) -> Any:
    """Load JSON file."""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_json(data: Any, filepath: str) -> None:
    """Save data to JSON file."""
    os.makedirs(os.path.dirname(filepath) or '.', exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4)
    print(f"  Saved: {filepath}")


def run_pareval_evaluation(args) -> List[Dict]:
    """
    Run ParEval evaluation on generated code.
    
    This function imports and uses ParEval's driver system to compile and run
    the generated code.
    """
    # Update global DRIVERS_DIR if custom path provided
    global DRIVERS_DIR
    DRIVERS_DIR = args.drivers_dir
    
    # Setup imports from local drivers directory
    setup_local_imports()
    
    # Import ParEval modules from local drivers directory
    from cpp.cpp_driver_wrapper import CppDriverWrapper
    from util import load_json as pareval_load_json
    
    print("\n" + "=" * 60)
    print("Running ParEval Evaluation")
    print("=" * 60)
    
    # Load input data
    print(f"\nLoading generated code from {args.input}")
    data = load_json(args.input)
    print(f"  Loaded {len(data)} prompts")
    
    # Load ParEval configs
    print(f"\nLoading ParEval configurations...")
    
    # Use local launch configs by default (no SLURM/srun dependency)
    if args.use_slurm:
        launch_configs = pareval_load_json(args.launch_configs)
        print(f"  Launch configs: {args.launch_configs} (SLURM mode)")
    else:
        launch_configs = LOCAL_LAUNCH_CONFIGS
        print(f"  Launch configs: LOCAL (no SLURM/srun)")
    
    build_configs = pareval_load_json(args.build_configs)
    problem_sizes = pareval_load_json(args.problem_sizes)
    print(f"  Build configs: {args.build_configs}")
    print(f"  Problem sizes: {args.problem_sizes}")
    
    # Setup logging
    numeric_level = getattr(logging, args.log_level.upper(), None)
    logging.basicConfig(
        format="%(asctime)s [%(levelname)s] -- %(message)s",
        level=numeric_level
    )
    
    # Driver root path is now the local drivers directory
    driver_root = args.drivers_dir
    
    print(f"\n  Driver root: {driver_root}")
    print(f"  Build timeout: {args.build_timeout}s")
    print(f"  Run timeout: {args.run_timeout}s")
    
    # Process each prompt
    print(f"\nEvaluating {len(data)} prompts...")
    
    from tqdm import tqdm
    
    # Save current directory and change to driver root
    original_dir = os.getcwd()
    os.chdir(driver_root)
    
    try:
        for prompt in tqdm(data, desc="Evaluating"):
            parallelism_model = prompt.get('parallelism_model', 'cuda')
            
            # Check if already has results
            if 'outputs' in prompt and len(prompt['outputs']) > 0:
                first_output = prompt['outputs'][0]
                if isinstance(first_output, dict) and 'did_build' in first_output:
                    logging.debug(f"Skipping {prompt['name']} - already has results")
                    continue
            
            # Create driver wrapper
            driver = CppDriverWrapper(
                parallelism_model=parallelism_model,
                launch_configs=launch_configs,
                build_configs=build_configs,
                problem_sizes=problem_sizes,
                scratch_dir=args.scratch_dir,
                build_timeout=args.build_timeout,
                run_timeout=args.run_timeout,
                display_build_errors=args.log_build_errors,
                display_runs=False,
                early_exit_runs=False,
                dry=False
            )
            
            # Run evaluation
            driver.test_all_outputs_in_prompt(prompt)
    finally:
        # Restore original directory
        os.chdir(original_dir)
    
    # Save results
    print(f"\nSaving results to {args.output}")
    save_json(data, args.output)
    
    return data


def compute_pass_at_k(n: int, c: int, k: int) -> float:
    """
    Compute pass@k metric.
    
    Args:
        n: Total number of samples
        c: Number of correct samples
        k: k value for pass@k
        
    Returns:
        pass@k probability
    """
    if n - c < k:
        return 1.0
    
    import numpy as np
    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))


def compute_metrics(data: List[Dict], k_values: List[int], model_name: str) -> Dict[str, Any]:
    """
    Compute evaluation metrics from results.
    
    Args:
        data: List of prompt results
        k_values: List of k values for pass@k
        model_name: Name of the model
        
    Returns:
        Dictionary of computed metrics
    """
    print("\n" + "=" * 60)
    print("Computing Metrics")
    print("=" * 60)
    
    metrics = {
        'model_name': model_name,
        'timestamp': datetime.now().isoformat(),
        'num_prompts': len(data),
        'k_values': k_values,
        'by_problem_type': {},
        'overall': {}
    }
    
    # Aggregate statistics
    total_outputs = 0
    total_builds = 0
    total_runs = 0
    total_valid = 0
    
    problem_type_stats = {}
    
    for prompt in data:
        problem_type = prompt.get('problem_type', 'unknown')
        outputs = prompt.get('outputs', [])
        
        if problem_type not in problem_type_stats:
            problem_type_stats[problem_type] = {
                'num_prompts': 0,
                'num_outputs': 0,
                'num_builds': 0,
                'num_runs': 0,
                'num_valid': 0,
                'prompts': []
            }
        
        stats = problem_type_stats[problem_type]
        stats['num_prompts'] += 1
        
        prompt_stats = {
            'name': prompt.get('name', 'unknown'),
            'num_outputs': 0,
            'num_builds': 0,
            'num_runs': 0,
            'num_valid': 0
        }
        
        for output in outputs:
            if isinstance(output, dict):
                prompt_stats['num_outputs'] += 1
                if output.get('did_build', False):
                    prompt_stats['num_builds'] += 1
                if output.get('did_any_run', False):
                    prompt_stats['num_runs'] += 1
                if output.get('are_any_valid', False):
                    prompt_stats['num_valid'] += 1
            elif isinstance(output, str):
                # Output hasn't been evaluated yet
                prompt_stats['num_outputs'] += 1
        
        stats['num_outputs'] += prompt_stats['num_outputs']
        stats['num_builds'] += prompt_stats['num_builds']
        stats['num_runs'] += prompt_stats['num_runs']
        stats['num_valid'] += prompt_stats['num_valid']
        stats['prompts'].append(prompt_stats)
        
        total_outputs += prompt_stats['num_outputs']
        total_builds += prompt_stats['num_builds']
        total_runs += prompt_stats['num_runs']
        total_valid += prompt_stats['num_valid']
    
    # Compute pass@k for each problem type
    for problem_type, stats in problem_type_stats.items():
        type_metrics = {
            'num_prompts': stats['num_prompts'],
            'num_outputs': stats['num_outputs'],
            'num_builds': stats['num_builds'],
            'num_runs': stats['num_runs'],
            'num_valid': stats['num_valid'],
            'build_rate': stats['num_builds'] / max(stats['num_outputs'], 1) * 100,
            'run_rate': stats['num_runs'] / max(stats['num_outputs'], 1) * 100,
            'valid_rate': stats['num_valid'] / max(stats['num_outputs'], 1) * 100,
        }
        
        # Compute pass@k for this problem type
        for k in k_values:
            pass_at_k_values = []
            build_at_k_values = []
            
            for prompt_stats in stats['prompts']:
                n = prompt_stats['num_outputs']
                if n > 0:
                    c_valid = prompt_stats['num_valid']
                    c_build = prompt_stats['num_builds']
                    
                    if k <= n:
                        pass_at_k_values.append(compute_pass_at_k(n, c_valid, k))
                        build_at_k_values.append(compute_pass_at_k(n, c_build, k))
            
            if pass_at_k_values:
                type_metrics[f'pass@{k}'] = sum(pass_at_k_values) / len(pass_at_k_values) * 100
                type_metrics[f'build@{k}'] = sum(build_at_k_values) / len(build_at_k_values) * 100
        
        metrics['by_problem_type'][problem_type] = type_metrics
    
    # Compute overall metrics
    overall = {
        'num_prompts': len(data),
        'num_outputs': total_outputs,
        'num_builds': total_builds,
        'num_runs': total_runs,
        'num_valid': total_valid,
        'build_rate': total_builds / max(total_outputs, 1) * 100,
        'run_rate': total_runs / max(total_outputs, 1) * 100,
        'valid_rate': total_valid / max(total_outputs, 1) * 100,
    }
    
    # Compute overall pass@k
    for k in k_values:
        pass_at_k_values = []
        build_at_k_values = []
        
        for problem_type, stats in problem_type_stats.items():
            for prompt_stats in stats['prompts']:
                n = prompt_stats['num_outputs']
                if n > 0 and k <= n:
                    c_valid = prompt_stats['num_valid']
                    c_build = prompt_stats['num_builds']
                    pass_at_k_values.append(compute_pass_at_k(n, c_valid, k))
                    build_at_k_values.append(compute_pass_at_k(n, c_build, k))
        
        if pass_at_k_values:
            overall[f'pass@{k}'] = sum(pass_at_k_values) / len(pass_at_k_values) * 100
            overall[f'build@{k}'] = sum(build_at_k_values) / len(build_at_k_values) * 100
    
    metrics['overall'] = overall
    
    return metrics


def print_metrics(metrics: Dict[str, Any]) -> None:
    """Print metrics in a formatted way."""
    print("\n" + "=" * 60)
    print(f"Evaluation Results: {metrics['model_name']}")
    print("=" * 60)
    
    overall = metrics['overall']
    k_values = metrics['k_values']
    
    print(f"\nOverall Statistics:")
    print(f"  Prompts evaluated: {overall['num_prompts']}")
    print(f"  Total outputs: {overall['num_outputs']}")
    print(f"  Successful builds: {overall['num_builds']} ({overall['build_rate']:.1f}%)")
    print(f"  Successful runs: {overall['num_runs']} ({overall['run_rate']:.1f}%)")
    print(f"  Valid outputs: {overall['num_valid']} ({overall['valid_rate']:.1f}%)")
    
    print(f"\nPass@k Metrics:")
    for k in k_values:
        build_k = overall.get(f'build@{k}', 0)
        pass_k = overall.get(f'pass@{k}', 0)
        print(f"  build@{k}: {build_k:.2f}%")
        print(f"  pass@{k}: {pass_k:.2f}%")
    
    print(f"\nBy Problem Type:")
    print("-" * 60)
    
    for problem_type, type_metrics in metrics['by_problem_type'].items():
        print(f"\n  {problem_type}:")
        print(f"    Prompts: {type_metrics['num_prompts']}")
        print(f"    Build rate: {type_metrics['build_rate']:.1f}%")
        print(f"    Valid rate: {type_metrics['valid_rate']:.1f}%")
        for k in k_values:
            if f'pass@{k}' in type_metrics:
                print(f"    pass@{k}: {type_metrics[f'pass@{k}']:.2f}%")
    
    print("\n" + "=" * 60)


def main():
    """Main entry point."""
    args = parse_args()
    
    print("\n" + "=" * 60)
    print("MoT ParEval Evaluation")
    print("=" * 60)
    print(f"  Input: {args.input}")
    print(f"  Output: {args.output}")
    print(f"  Metrics output: {args.metrics_output}")
    print(f"  Drivers directory: {args.drivers_dir}")
    print(f"  K values: {args.k_values}")
    print(f"  Model name: {args.model_name}")
    print("=" * 60)
    
    # Check input file exists
    if not os.path.exists(args.input):
        print(f"\nError: Input file not found: {args.input}")
        sys.exit(1)
    
    if args.metrics_only:
        # Only compute metrics from existing results
        print("\nMetrics-only mode: Loading existing results...")
        data = load_json(args.input)
    else:
        # Run full evaluation
        data = run_pareval_evaluation(args)
    
    if not args.skip_metrics:
        # Compute metrics
        metrics = compute_metrics(data, args.k_values, args.model_name)
        
        # Print metrics
        print_metrics(metrics)
        
        # Save metrics
        print(f"\nSaving metrics to {args.metrics_output}")
        save_json(metrics, args.metrics_output)
    
    print("\n" + "=" * 60)
    print("Evaluation Complete")
    print("=" * 60)


if __name__ == '__main__':
    main()